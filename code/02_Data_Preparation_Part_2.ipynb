{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb44086",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3658f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff676794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to load entire text file into memory and return it.\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r', \n",
    "                encoding='utf-8')    # open the file as read only\n",
    "    text = file.read()               # read all the text\n",
    "    file.close()                     # close the file\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10b90675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿I\n",
      "\n",
      "One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "himself transformed in his bed into a horrible vermin. He lay on his\n",
      "armour-like back, and if he lifted his head a little he could see his\n",
      "brown belly, slightly domed and divided by arches into stiff sections.\n",
      "The bedding was hardly able to cover it and seemed ready to slide off\n",
      "any moment. His many legs, pitifully thin compared with the size of the\n",
      "rest of him, waved about helplessly as he looked.\n",
      "\n",
      "“What’s happened to me?” he thought. It wasn’t a dream. His room, a\n",
      "proper human room although a little too small, lay peacefully between\n",
      "its four familiar walls. A collection of textile samples lay spread out\n",
      "on the table—Samsa was a travelling salesman—and above it there hung a\n",
      "picture that he had recently cut out of an illustrated magazine and\n",
      "housed in a nice, gilded frame. It showed a lady fitted out with a fur\n",
      "hat and fur boa who sat upright, raising a heavy fur muff that covered\n",
      "the whole of her lower arm towards \n"
     ]
    }
   ],
   "source": [
    "# load Metamorphosis text, meta_text.txt \n",
    "in_filename = '../data/meta_text.txt'\n",
    "doc = load_doc(in_filename)\n",
    "\n",
    "#preview first 1000 characters \n",
    "print(doc[:1000]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c68ebb",
   "metadata": {},
   "source": [
    "### 2.3 Clean Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aad3f55",
   "metadata": {},
   "source": [
    "To transform the raw text into a series of tokens to train the model, several operations will be performed.\n",
    "- Replace '-' with a white space so that the words can be split better\n",
    "- Split the words based on white space\n",
    "- Remove all punctuation from words\n",
    "- Remove all words that are not alphabetic\n",
    "- Normalize all words to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f57433a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to convert document into tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '-' with ' ' \n",
    "    doc = doc.replace('-', ' ')\n",
    "    # split doc into tokens by space\n",
    "    tokens = doc.split()           \n",
    "    # filter punctuated chars\n",
    "    re_punc = re.compile('[%s]'% re.escape(string.punctuation)) \n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]  \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()] \n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00ad0aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morning', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'he', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'his', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'happened', 'to', 'he', 'thought', 'it', 'a', 'dream', 'his', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small', 'lay', 'peacefully', 'between', 'its', 'four', 'familiar', 'walls', 'a', 'collection', 'of', 'textile', 'samples', 'lay', 'spread', 'out', 'on', 'the', 'was', 'a', 'travelling', 'above', 'it', 'there', 'hung', 'a', 'picture', 'that', 'he', 'had', 'recently', 'cut', 'out', 'of', 'an', 'illustrated', 'magazine', 'and', 'housed', 'in', 'a', 'nice', 'gilded', 'frame', 'it', 'showed', 'a', 'lady', 'fitted', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sat', 'upright', 'raising', 'a', 'heavy', 'fur', 'muff', 'that', 'covered', 'the', 'whole', 'of', 'her', 'lower', 'arm', 'towards', 'the', 'viewer', 'gregor', 'then', 'turned', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather', 'drops', 'of', 'rain', 'could', 'be', 'heard', 'hitting', 'the', 'pane']\n"
     ]
    }
   ],
   "source": [
    "# convert document into tokens\n",
    "tokens = clean_doc(doc)\n",
    "# print first 200 tokens\n",
    "print(tokens[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed01589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens:  21347\n",
      "Number of unique tokens:  2533\n"
     ]
    }
   ],
   "source": [
    "# print total number of tokens \n",
    "print('Total number of tokens: ', len(tokens))\n",
    "# convert tokens into a set and print the number of unique tokens\n",
    "print('Number of unique tokens: ', len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4697b9",
   "metadata": {},
   "source": [
    "### 2.4 Save Clean Text as Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cdd6f9",
   "metadata": {},
   "source": [
    "#### Sequence of 50 + 1 Words\n",
    "For the reference model, we will train the model using a sequence length of 50 words. The tokens will be organized into sequences of 50 input words + 1 output word, creating a sequence of 51 words. The list of tokens will be iterated from token 51 onwards with the prior 50 tokens taken as a sequence. The sequence of tokens will be saved as space separated strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70acbadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 21296\n"
     ]
    }
   ],
   "source": [
    "# organize token list into sequences\n",
    "length = 50+1\n",
    "# replace list() with []\n",
    "sequences = list()\n",
    "\n",
    "for i in range(51, 21347):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    #convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store each line in sequences\n",
    "    sequences.append(line)\n",
    "    \n",
    "# print total number of sequences\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "274d2397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f42268d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = '../data/Text_Sequences_50_meta.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b997164",
   "metadata": {},
   "source": [
    "#### Sequence of 100 + 1 Words\n",
    "We would like to explore if using a longer sequence length will improve the model performance. While we found that the average paragraph length was approximately 200 words, the sequence length of 200 words will require a much longer time to train the model. Hence, we will use a sequence length of 100 words to observe the effect of longer sequence length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb56f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 21246\n"
     ]
    }
   ],
   "source": [
    "# organize token list into sequences\n",
    "length = 100+1\n",
    "sequences = list()\n",
    "\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    #convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store each line in sequences\n",
    "    sequences.append(line)\n",
    "    \n",
    "# print total number of sequences\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f883979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = '../data/Text_Sequences_100_meta.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b40d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
