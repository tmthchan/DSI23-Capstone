{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce80fe9",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70692484",
   "metadata": {},
   "source": [
    "### 2.8 Load Text - Kafka on the Shore by Haruki Murakami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c3b17c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to load entire text file into memory and return it.\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r', \n",
    "                encoding='utf-8')    # open the file as read only\n",
    "    text = file.read()               # read all the text\n",
    "    file.close()                     # close the file\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d2ecdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Cash isn’t the only thing I take from my father’s study when I leave home. I take a small, old gold lighter—I like the design and feel of it—and a folding knife with a really sharp blade. Made to skin deer, it has a five-inch blade and a nice heft. \n"
     ]
    }
   ],
   "source": [
    "# load document, Text_Cleaned.txt \n",
    "in_filename = '../data/kots_text.txt'\n",
    "doc = load_doc(in_filename)\n",
    "\n",
    "#preview first 250 characters \n",
    "print(doc[:250])                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39ce65",
   "metadata": {},
   "source": [
    "### 2.9 Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f87f7645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56a8f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to convert document into tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '-' with ' ' \n",
    "    doc = doc.replace('-', ' ')\n",
    "    # split doc into tokens by space\n",
    "    tokens = doc.split()           \n",
    "    # filter punctuated chars\n",
    "    re_punc = re.compile('[%s]'% re.escape(string.punctuation)) \n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]  \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()] \n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f98e888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'only', 'thing', 'i', 'take', 'from', 'my', 'study', 'when', 'i', 'leave', 'home', 'i', 'take', 'a', 'small', 'old', 'gold', 'like', 'the', 'design', 'and', 'feel', 'of', 'a', 'folding', 'knife', 'with', 'a', 'really', 'sharp', 'blade', 'made', 'to', 'skin', 'deer', 'it', 'has', 'a', 'five', 'inch', 'blade', 'and', 'a', 'nice', 'heft', 'probably', 'something', 'he', 'bought', 'on', 'one', 'of', 'his', 'trips', 'abroad', 'i', 'also', 'take', 'a', 'sturdy', 'bright', 'pocket', 'flashlight', 'out', 'of', 'a', 'drawer', 'plus', 'sky', 'blue', 'revo', 'sunglasses', 'to', 'disguise', 'my', 'age', 'i', 'think', 'about', 'taking', 'my', 'favorite', 'sea', 'dweller', 'oyster', 'rolex', 'a', 'beautiful', 'watch', 'but', 'something', 'flashy', 'will', 'only', 'attract', 'attention', 'my', 'cheap', 'plastic', 'casio', 'watch', 'with', 'an', 'alarm', 'and', 'stopwatch', 'will', 'do', 'just', 'fine', 'and', 'might', 'actually', 'be', 'more', 'useful', 'reluctantly', 'i', 'return', 'the', 'rolex', 'to', 'its', 'drawer', 'the', 'back', 'of', 'another', 'drawer', 'i', 'take', 'out', 'a', 'photo', 'of', 'me', 'and', 'my', 'older', 'sister', 'when', 'we', 'were', 'little', 'the', 'two', 'of', 'us', 'on', 'a', 'beach', 'somewhere', 'with', 'grins', 'plastered', 'across', 'our', 'faces', 'my', 'looking', 'off', 'to', 'the', 'side', 'so', 'half', 'her', 'face', 'is', 'in', 'shadow', 'and', 'her', 'smile', 'is', 'neatly', 'cut', 'in', 'half', 'like', 'one', 'of', 'those', 'greek', 'tragedy', 'masks', 'in', 'a', 'textbook', 'half', 'one', 'idea', 'and', 'half', 'the', 'opposite', 'light', 'and', 'dark']\n"
     ]
    }
   ],
   "source": [
    "# convert document into tokens\n",
    "tokens = clean_doc(doc)\n",
    "# print first 200 tokens\n",
    "print(tokens[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "573323ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens:  661\n",
      "Number of unique tokens:  328\n"
     ]
    }
   ],
   "source": [
    "# print total number of tokens \n",
    "print('Total number of tokens: ', len(tokens))\n",
    "# convert tokens into a set and print the number of unique tokens\n",
    "print('Number of unique tokens: ', len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd362f",
   "metadata": {},
   "source": [
    "### 2.10 Save Clean Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24389336",
   "metadata": {},
   "source": [
    "#### Sequence of 50 + 1 Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "031c0bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 610\n"
     ]
    }
   ],
   "source": [
    "# organize token list into sequences\n",
    "length = 50+1\n",
    "sequences = list()\n",
    "\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    #convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store each line in sequences\n",
    "    sequences.append(line)\n",
    "    \n",
    "# print total number of sequences\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a92ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eaee286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = '../data/Text_Sequences_50_kots.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e4ffc",
   "metadata": {},
   "source": [
    "#### Sequence of 100 + 1 Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36239e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 560\n"
     ]
    }
   ],
   "source": [
    "# organize token list into sequences\n",
    "length = 100+1\n",
    "sequences = list()\n",
    "\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    #convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store each line in sequences\n",
    "    sequences.append(line)\n",
    "    \n",
    "# print total number of sequences\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdc73af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = '../data/Text_Sequences_100_kots.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ceecd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
